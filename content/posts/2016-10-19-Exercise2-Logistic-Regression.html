---
title: Exercise 2 - Logistic Regression (1)
date: 2016-10-19
tags: [Machine learning, Stanford, Assignment]
---



<div id="logistic-regression" class="section level1">
<h1>1 Logistic Regression</h1>
<div id="visualizing-data" class="section level2">
<h2>1.1 Visualizing data</h2>
<div id="plotdata.m" class="section level3">
<h3>plotdata.m</h3>
<pre class="m"><code>function plotData(X, y)
%PLOTDATA Plots the data points X and y into a new figure 
%   PLOTDATA(x,y) plots the data points with + for the positive examples
%   and o for the negative examples. X is assumed to be a Mx2 matrix.

% Create New Figure
figure; hold on;

% ====================== YOUR CODE HERE ======================
% Instructions: Plot the positive and negative examples on a
%               2D plot, using the option &#39;k+&#39; for the positive
%               examples and &#39;ko&#39; for the negative examples.
%


% Find Indices of Positive and Negative Examples
pos = find(y==1); neg = find(y == 0);
% Plot Examples
plot(X(pos, 1), X(pos, 2), &#39;k+&#39;,&#39;LineWidth&#39;, 2, &#39;MarkerSize&#39;, 7);
plot(X(neg, 1), X(neg, 2), &#39;ko&#39;, &#39;MarkerFaceColor&#39;, &#39;y&#39;, ...
    &#39;MarkerSize&#39;, 7);

% =========================================================================

hold off;

end</code></pre>
</div>
</div>
<div id="sigmoid-function" class="section level2">
<h2>1.2 Sigmoid function</h2>
<p>Logistic regression hypothesis is defined as: <span class="math display">\[h_\theta(x) = g(\theta^Tx)\]</span>, where function <span class="math inline">\(g\)</span> is the sigmoid function. The sigmoid function is defined as: <span class="math display">\[g(z) = \frac{1}{1 + e^{-z}}\]</span></p>
<div id="sigmoid.m" class="section level3">
<h3>Sigmoid.m</h3>
<pre class="m"><code>function g = sigmoid(z)
%SIGMOID Compute sigmoid functoon
%   J = SIGMOID(z) computes the sigmoid of z.

% You need to return the following variables correctly 
g = zeros(size(z));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the sigmoid of each value of z (z can be a matrix,
%               vector or scalar).

g = 1./(1 .+ exp(-1*z) );

% =============================================================

end</code></pre>
</div>
</div>
<div id="cost-function-and-gradient" class="section level2">
<h2>1.3 Cost function and gradient</h2>
<p>The cost function in logistic regression is <span class="math display">\[J(\theta) = \frac{1}{m}\sum_{i=1}^m[−y^{(i)}\log(h_\theta(x^{(i)})) − (1 − y^{(i)})\log(1 − h_\theta(x^{(i)}))]\]</span>, and the gradient of the cost is a vector of the same length as <span class="math inline">\(\theta\)</span> where the <span class="math inline">\(j\)</span>th element (for j = 0, 1, . . . , n) is defined as follows: <span class="math display">\[\frac{\partial J(\theta) }{\partial \theta_j } = \frac{1}{m}\sum_{i=1}^m \bigg(h_\theta(x^{(i)}) − y^{(i)}\bigg)x_j^{(i)}\]</span></p>
<div id="constfunction.m" class="section level3">
<h3>constFunction.m</h3>
<pre class="m"><code>function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;
grad = zeros(size(theta));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta
%
% Note: grad should have the same dimensions as theta
%

h = sigmoid(X*theta);
J=(-y&#39;*log(h) - (1-y)&#39;*log(1.-h))/m;
grad = X&#39;*(h-y)/m;


% =============================================================

end</code></pre>
</div>
</div>
<div id="learning-parameters-using-fminunc" class="section level2">
<h2>1.4 Learning parameters using fminunc</h2>
<p>Octave/MATLAB’s fminunc is an optimization solver that finds the minimum of an unconstrained function. For logistic regression, you want to optimize the cost function <span class="math inline">\(J(\theta)\)</span> with parameters <span class="math inline">\(\theta\)</span>.</p>
<pre class="m"><code>%% ============= Part 3: Optimizing using fminunc  =============
%  In this exercise, you will use a built-in function (fminunc) to find the
%  optimal parameters theta.

%  Set options for fminunc
options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 400);

%  Run fminunc to obtain the optimal theta
%  This function will return theta and the cost 
[theta, cost] = ...
    fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);</code></pre>
</div>
<div id="decision-boundary" class="section level2">
<h2>1.5 Decision boundary</h2>
<p>This final <span class="math inline">\(\theta\)</span> value computed from <strong>fminunc</strong> will then be used to plot the decision boundary on the training data.</p>
<p><span class="math inline">\(y\)</span> value on the decision boundary satifies: <span class="math display">\[y = h_\theta(x) = g\bigg(\theta^Tx\bigg) = 0.5 \]</span>, that is, <span class="math display">\[\theta^Tx = 0 \]</span></p>
<ul>
<li><p>When training data X has two features <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math display">\[\theta_1 + \theta_2 * x_1 + \theta_3*x_2 = 0 \]</span>, that is, <span class="math display">\[x_2 = -\frac{\theta_1 + \theta_2 * x_1}{\theta_3} \]</span>,</p></li>
<li><p>When training data X has more than 2 features, how to visualize it on 2D plot?</p></li>
</ul>
<div id="plotdecisionboundary.m" class="section level3">
<h3>plotDecisionBoundary.m</h3>
<pre class="m"><code>function plotDecisionBoundary(theta, X, y)
%PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with
%the decision boundary defined by theta
%   PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the 
%   positive examples and o for the negative examples. X is assumed to be 
%   a either 
%   1) Mx3 matrix, where the first column is an all-ones column for the 
%      intercept.
%   2) MxN, N&gt;3 matrix, where the first column is all-ones

% Plot Data
plotData(X(:,2:3), y);
hold on

if size(X, 2) &lt;= 3
    % Only need 2 points to define a line, so choose two endpoints
    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];

    % Calculate the decision boundary line
    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));

    % Plot, and adjust axes for better viewing
    plot(plot_x, plot_y)
    
    % Legend, specific for the exercise
    legend(&#39;Admitted&#39;, &#39;Not admitted&#39;, &#39;Decision Boundary&#39;)
    axis([30, 100, 30, 100])
else
    % Here is the grid range
    u = linspace(-1, 1.5, 50);
    v = linspace(-1, 1.5, 50);

    z = zeros(length(u), length(v));
    % Evaluate z = theta*x over the grid
    for i = 1:length(u)
        for j = 1:length(v)
            z(i,j) = mapFeature(u(i), v(j))*theta;
        end
    end
    z = z&#39;; % important to transpose z before calling contour

    % Plot z = 0
    % Notice you need to specify the range [0, 0]
    contour(u, v, z, [0, 0], &#39;LineWidth&#39;, 2)
end
hold off

end
</code></pre>
</div>
<div id="mapfeature.m" class="section level3">
<h3>mapFeature.m</h3>
<pre class="m"><code>function out = mapFeature(X1, X2)
% MAPFEATURE Feature mapping function to polynomial features
%
%   MAPFEATURE(X1, X2) maps the two input features
%   to quadratic features used in the regularization exercise.
%
%   Returns a new feature array with more features, comprising of 
%   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..
%
%   Inputs X1, X2 must be the same size
%

degree = 6;
out = ones(size(X1(:,1)));
for i = 1:degree
    for j = 0:i
        out(:, end+1) = (X1.^(i-j)).*(X2.^j);
    end
end

end</code></pre>
</div>
</div>
<div id="evaluating-logistic-regression" class="section level2">
<h2>1.6 Evaluating logistic regression</h2>
<div id="predict.m" class="section level3">
<h3>predict.m</h3>
<pre class="m"><code>function p = predict(theta, X)
%PREDICT Predict whether the label is 0 or 1 using learned logistic 
%regression parameters theta
%   p = PREDICT(theta, X) computes the predictions for X using a 
%   threshold at 0.5 (i.e., if sigmoid(theta&#39;*x) &gt;= 0.5, predict 1)

m = size(X, 1); % Number of training examples

% You need to return the following variables correctly
p = zeros(m, 1);

% ====================== YOUR CODE HERE ======================
% Instructions: Complete the following code to make predictions using
%               your learned logistic regression parameters. 
%               You should set p to a vector of 0&#39;s and 1&#39;s
%

p = sigmoid(X, theta)&gt;=0.5;

% =========================================================================

end</code></pre>
<pre class="m"><code>%  Predict probability for a student with score 45 on exam 1 
%  and score 85 on exam 2 

prob = sigmoid([1 45 85] * theta);
fprintf([&#39;For a student with scores 45 and 85, we predict an admission &#39; ...
         &#39;probability of %f\n\n&#39;], prob);

% Compute accuracy on our training set
p = predict(theta, X);

fprintf(&#39;Train Accuracy: %f\n&#39;, mean(double(p == y)) * 100);</code></pre>
</div>
</div>
</div>
