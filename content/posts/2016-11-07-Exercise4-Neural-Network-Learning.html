---
title: Exercise 4 - Neural Network Learning
date: 2016-11-07
tags: [Machine learning, Stanford, Assignment]
---



<div id="neural-networks" class="section level1">
<h1>1. Neural Networks</h1>
<div id="feedforward-and-const-function" class="section level2">
<h2>Feedforward and const function</h2>
<p>The cost function for the neural network (without regularization) is</p>
<p><span class="math display">\[
J(\theta) = \frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K
\bigg[
−y_k^{(i)}\log((h_{θ}(x^{(i)}))_k)−(1−y_k^{(i)})\log(1−(h_θ(x^{(i)}))_k)
\bigg]
\]</span>,</p>
<p>where <span class="math inline">\(h_{\theta}(x^{(i)})\)</span> is computed as shown in the Figure 2 and <span class="math inline">\(K = 10\)</span> is the total number of possible labels. Note that <span class="math inline">\(h_θ(x^{(i)})_k = a^{(3)}_k\)</span> is the activation (output value) of the <span class="math inline">\(k\)</span>-th output unit.</p>
<p><strong>Implementation-nnCostFunction.m</strong></p>
<pre class="m"><code>a1 = [ones(m, 1) X];
z2 = a1*Theta1&#39;;
a2 = [ones(size(z2, 1), 1) sigmoid(z2)];
z3 = a2*Theta2&#39;;
a3 = sigmoid(z3);

yd = eye(num_labels);
y = yd(y,:);

log_dif = -log(a3).*y-log(1-a3).*(1-y);
J=sum(log_dif(:))/m;
</code></pre>
</div>
<div id="regularized-const-function" class="section level2">
<h2>Regularized const function</h2>
<p>The cost function for neural networks with regularization is given by</p>
<p><span class="math display">\[
J(\theta) = \frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K
\bigg[
−y_k^{(i)}\log((h_{θ}(x^{(i)}))_k)−(1−y_k^{(i)})\log(1−(h_θ(x^{(i)}))_k)
\bigg]
\]</span></p>
<p><span class="math display">\[
+ \frac{\lambda}{2m}\bigg[\sum_{j=1}^{25}\sum_{k=1}^{400}(\theta_{j,k}^{(1)})^2+\sum_{j=1}^{10}\sum_{k=1}^{25}(\theta_{j,k}^{(2)})^2\bigg]
\]</span></p>
<p><strong>Implementation-nnCostFunction.m</strong></p>
<pre class="m"><code>a1 = [ones(m, 1) X];
z2 = a1*Theta1&#39;;
a2 = [ones(size(z2, 1), 1) sigmoid(z2)];
z3 = a2*Theta2&#39;;
a3 = sigmoid(z3);

yd = eye(num_labels);
y = yd(y,:);

log_dif = -log(a3).*y-log(1-a3).*(1-y);

Theta1s=Theta1(:,2:end);
Theta2s=Theta2(:,2:end);

penalty = lambda/(2*m)*(sum((Theta1s.*Theta1s)(:)) + sum((Theta2s.*Theta2s)(:)));
J=sum(log_dif(:))/m + penalty;</code></pre>
</div>
</div>
<div id="backpropagation" class="section level1">
<h1>2. Backpropagation</h1>
<div id="section" class="section level2">
<h2></h2>
</div>
</div>
